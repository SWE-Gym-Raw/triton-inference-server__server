# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
FROM nvcr.io/nvidia/tritonserver:24.11-vllm-python-py3

WORKDIR /opt/tritonserver

# Clone OpenAI-compatible server code and place in container
RUN git clone --single-branch --depth=1 -b r24.11 https://github.com/triton-inference-server/server.git && \
    mv server/python/openai openai && \
    chown -R root:root openai && \
    chmod 755 openai && \
    chmod -R go-w openai && \
    rm -rf server

# Install OpenAI-compatible server dependencies
RUN python3 -m pip install "openai==1.40.6" "fastapi==0.111.1"

# OPTIONAL: Add sample model repository for vLLM model for quickstart.
# NOTE: Choosing a small model with a chat template and public access, and
#       pre-downloading it into container for testing convenience.
# NOTE: This section should be removed or modified based on deployment needs.
RUN huggingface-cli download "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
RUN git clone --single-branch --depth=1 -b r24.11 https://github.com/triton-inference-server/vllm_backend.git && \
    mv vllm_backend/samples/model_repository sample_model_repository && \
    sed -i 's/"facebook\/opt-125m"/"TinyLlama\/TinyLlama-1.1B-Chat-v1.0"/' sample_model_repository/vllm_model/1/model.json && \
    chown -R root:root sample_model_repository && \
    chmod 755 sample_model_repository && \
    chmod -R go-w sample_model_repository && \
    rm -rf vllm_backend
